{
  "id": 130,
  "title": "restart_funkit",
  "body": "#!/usr/bin/env bash\nset -Eeuo pipefail\n\n# --- EDIT THESE IF YOU WANT ---\nMODEL_FILE=\"${HOME}/localai/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\nMODEL_ALIAS=\"mistral-7b-instruct\"   # shows up under /v1/models\nPORT=\"8081\"\nAPI_KEY=\"sk-local\"\n\n# Optional server tuning (safe defaults)\nCTX_SIZE=\"${CTX_SIZE:-4096}\"\nN_THREADS=\"${N_THREADS:-$(nproc)}\"\n\n# --- Find the llama.cpp server binary ---\nSERVER_BIN=\"$(command -v server || true)\"\nif [[ -z \"$SERVER_BIN\" ]]; then\n  # common local build path\n  if [[ -x \"${HOME}/llama.cpp/server\" ]]; then\n    SERVER_BIN=\"${HOME}/llama.cpp/server\"\n  fi\nfi\nif [[ ! -x \"${SERVER_BIN:-/nonexistent}\" ]]; then\n  echo \"\u274c Could not find the llama.cpp server binary.\"\n  echo \"   Put 'server' in your PATH, or build it at ~/llama.cpp/server.\"\n  exit 1\nfi\n\n# --- Sanity checks ---\nif [[ ! -f \"$MODEL_FILE\" ]]; then\n  echo \"\u274c Model file not found: $MODEL_FILE\"\n  echo \"   Update MODEL_FILE at the top of this script.\"\n  exit 1\nfi\n\necho \"\ud83d\uded1 Stopping any existing llama.cpp server on port ${PORT} (best-effort)...\"\npkill -f \"server .*--port ${PORT}\" 2>/dev/null || true\npkill -f \"server --port ${PORT}\" 2>/dev/null || true\nsleep 0.3\n\necho \"\ud83d\ude80 Starting llama.cpp server\u2026\"\necho \"   Binary : $SERVER_BIN\"\necho \"   Model  : $MODEL_FILE\"\necho \"   Alias  : $MODEL_ALIAS\"\necho \"   Port   : $PORT\"\necho \"   API key: (set)\"\n\n# Start the server in the background\n# Notes:\n#  --alias sets the ID returned by /v1/models\n#  --api-key requires Authorization: Bearer <API_KEY>\n#  --ctx-size and --threads are optional tuning knobs\n\"$SERVER_BIN\" \\\n  --model \"$MODEL_FILE\" \\\n  --alias \"$MODEL_ALIAS\" \\\n  --host 0.0.0.0 \\\n  --port \"$PORT\" \\\n  --api-key \"$API_KEY\" \\\n  --ctx-size \"$CTX_SIZE\" \\\n  --threads \"$N_THREADS\" \\\n  >/tmp/llama_server.log 2>&1 &\n\n# Wait for the server to come up\nBASE_URL=\"http://localhost:${PORT}/v1\"\necho -n \"\u23f3 Waiting for ${BASE_URL}/models \u2026\"\nfor i in {1..40}; do\n  code=$(curl -s -o /dev/null -w \"%{http_code}\" -H \"Authorization: Bearer ${API_KEY}\" \"${BASE_URL}/models\" || true)\n  if [[ \"$code\" == \"200\" ]]; then\n    echo \" up!\"\n    break\n  fi\n  echo -n \".\"\n  sleep 0.25\ndone\nif [[ \"$code\" != \"200\" ]]; then\n  echo -e \"\\n\u274c llama.cpp server didn't respond OK. Check /tmp/llama_server.log\"\n  exit 1\nfi\n\n# Verify that our alias shows up\nif ! curl -s -H \"Authorization: Bearer ${API_KEY}\" \"${BASE_URL}/models\" | grep -q \"\\\"id\\\":\\\"${MODEL_ALIAS}\\\"\"; then\n  echo \"\u26a0\ufe0f  Model alias '${MODEL_ALIAS}' not visible yet. Proceeding anyway.\"\nfi\n\n# --- FunKit env ---\nexport PIKIT_OPENAI_BASE_URL=\"${BASE_URL}\"\nexport PIKIT_OPENAI_API_KEY=\"${API_KEY}\"\nexport PIKIT_MODEL_NAME=\"${MODEL_ALIAS}\"\nexport PIKIT_MAX_TOKENS_DEFAULT=\"${PIKIT_MAX_TOKENS_DEFAULT:-256}\"\nunset PIKIT_FORCE_NO_AUTH\n\necho \"\u25b6 FunKit will use:\"\necho \"   BASE_URL: $PIKIT_OPENAI_BASE_URL\"\necho \"   MODEL   : $PIKIT_MODEL_NAME\"\necho \"   API KEY : (set)\"\necho \"   TOKENS  : $PIKIT_MAX_TOKENS_DEFAULT\"\n\n# Launch FunKit\ncd \"${HOME}/src/FunKit\"\nexec python3 main.py\n\n"
}