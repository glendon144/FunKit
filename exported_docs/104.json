{
  "id": 104,
  "title": "local_ai_interface",
  "body": "\n#!/usr/bin/env python3\n\"\"\"\nai_interface.py \u2014 OpenAI-compatible adapter for FunKit (LocalAI / vLLM / OpenAI)\n==============================================================================\n- Uses plain HTTP via 'requests' (no openai lib dependency)\n- Exposes ASK-first legacy methods directly on AIInterface:\n    ask(), query(), completions(), completion(), complete(), chat_completions()\n- Also exports TriAIInterface shim for older imports\n- Compatible with /v1/chat/completions and /v1/embeddings\n\nEnvironment (with defaults):\n  PIKIT_OPENAI_BASE_URL   default: http://localhost:8080/v1\n  PIKIT_OPENAI_API_KEY    default: sk-local\n  PIKIT_MODEL_NAME        default: mistral-7b-instruct\n  PIKIT_REQUEST_TIMEOUT   default: 120\n  PIKIT_CHAT_TEMPERATURE  default: 0.7\n\"\"\"\n\nfrom __future__ import annotations\nimport os, json, typing as t, requests\n\nJson = t.Dict[str, t.Any]\nHeaders = t.Dict[str, str]\n\nDEFAULTS = {\n    \"base_url\": os.getenv(\"PIKIT_OPENAI_BASE_URL\", \"http://localhost:8080/v1\").rstrip(\"/\"),\n    \"api_key\": os.getenv(\"PIKIT_OPENAI_API_KEY\", \"sk-local\"),\n    \"model\": os.getenv(\"PIKIT_MODEL_NAME\", \"mistral-7b-instruct\"),\n    \"timeout\": float(os.getenv(\"PIKIT_REQUEST_TIMEOUT\", \"120\")),\n    \"temperature\": float(os.getenv(\"PIKIT_CHAT_TEMPERATURE\", \"0.7\")),\n}\n\ndef _as_messages(user_or_messages: t.Union[str, t.List[Json]], system_prompt: t.Optional[str]) -> t.List[Json]:\n    if isinstance(user_or_messages, str):\n        msgs = [{\"role\": \"user\", \"content\": user_or_messages}]\n    else:\n        msgs = list(user_or_messages)\n    if system_prompt:\n        if not msgs or msgs[0].get(\"role\") != \"system\":\n            msgs = [{\"role\": \"system\", \"content\": system_prompt}] + msgs\n    return msgs\n\nclass AIInterface:\n    \"\"\"\n    Usage:\n        ai = AIInterface()\n        text = ai.ask(\"Say hi.\")              # legacy alias\n        text = ai.chat(\"Say hi.\")             # modern\n        for chunk in ai.ask(\"stream\", stream=True): print(chunk, end=\"\")\n        vecs = ai.embed([\"hello\", \"world\"])\n    \"\"\"\n    def __init__(self,\n                 base_url: str | None = None,\n                 api_key: str | None = None,\n                 model: str | None = None,\n                 timeout: float | None = None,\n                 default_temperature: float | None = None):\n        cfg = DEFAULTS.copy()\n        if base_url: cfg[\"base_url\"] = base_url.rstrip(\"/\")\n        if api_key: cfg[\"api_key\"] = api_key\n        if model: cfg[\"model\"] = model\n        if timeout is not None: cfg[\"timeout\"] = timeout\n        if default_temperature is not None: cfg[\"temperature\"] = default_temperature\n        self.base_url = cfg[\"base_url\"]\n        self.api_key = cfg[\"api_key\"]\n        self.model = cfg[\"model\"]\n        self.timeout = cfg[\"timeout\"]\n        self.default_temperature = cfg[\"temperature\"]\n        self._session = requests.Session()\n        self._system_prompt: str | None = None\n\n    # ---- config ----\n    def set_system_prompt(self, text: str | None) -> None:\n        self._system_prompt = text or None\n\n    def get_config(self) -> Json:\n        return {\n            \"base_url\": self.base_url,\n            \"model\": self.model,\n            \"timeout\": self.timeout,\n            \"default_temperature\": self.default_temperature,\n            \"has_system_prompt\": bool(self._system_prompt),\n        }\n\n    # ---- chat ----\n    def chat(self,\n             prompt_or_messages: t.Union[str, t.List[Json]],\n             stream: bool = False,\n             temperature: float | None = None,\n             max_tokens: int | None = None,\n             tools: t.Optional[t.List[Json]] = None,\n             tool_choice: t.Optional[t.Union[str, Json]] = None,\n             stop: t.Optional[t.Union[str, t.List[str]]] = None,\n             extra_headers: t.Optional[Headers] = None,\n             **extra) -> t.Union[str, t.Iterator[str]]:\n        url = f\"{self.base_url}/chat/completions\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        if extra_headers:\n            headers.update(extra_headers)\n        msgs = _as_messages(prompt_or_messages, self._system_prompt)\n        payload: Json = {\n            \"model\": self.model,\n            \"messages\": msgs,\n            \"temperature\": self.default_temperature if temperature is None else temperature,\n        }\n        if max_tokens is not None: payload[\"max_tokens\"] = max_tokens\n        if tools is not None: payload[\"tools\"] = tools\n        if tool_choice is not None: payload[\"tool_choice\"] = tool_choice\n        if stop is not None: payload[\"stop\"] = stop\n        if extra: payload.update(extra)\n\n        if stream:\n            payload[\"stream\"] = True\n            resp = self._session.post(url, headers=headers, json=payload, timeout=self.timeout, stream=True)\n            resp.raise_for_status()\n            return self._stream_text(resp)\n        else:\n            resp = self._session.post(url, headers=headers, json=payload, timeout=self.timeout)\n            return self._extract_text(resp)\n\n    # ---- embeddings ----\n    def embed(self, texts: t.List[str], model: str | None = None) -> t.List[t.List[float]]:\n        url = f\"{self.base_url}/embeddings\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        payload: Json = {\n            \"model\": model or self.model,\n            \"input\": texts,\n        }\n        resp = self._session.post(url, headers=headers, json=payload, timeout=self.timeout)\n        resp.raise_for_status()\n        data = resp.json()\n        return [item[\"embedding\"] for item in data.get(\"data\", [])]\n\n    # ---- internals ----\n    def _extract_text(self, resp: requests.Response) -> str:\n        try:\n            resp.raise_for_status()\n        except Exception as e:\n            raise RuntimeError(self._format_http_error(resp, e))\n        data = resp.json()\n        # Chat-style primary\n        try:\n            return data[\"choices\"][0][\"message\"][\"content\"] or \"\"\n        except Exception:\n            # Completion-style fallback\n            try:\n                return data[\"choices\"][0].get(\"text\", \"\") or \"\"\n            except Exception:\n                raise RuntimeError(f\"Unexpected response: {json.dumps(data)[:600]}\")\n\n    def _stream_text(self, resp: requests.Response) -> t.Iterator[str]:\n        for line in resp.iter_lines(decode_unicode=True):\n            if not line:\n                continue\n            payload = line[6:].strip() if line.startswith(\"data: \") else line.strip()\n            if payload == \"[DONE]\":\n                break\n            try:\n                j = json.loads(payload)\n                delta = j.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n                if delta:\n                    yield delta\n            except Exception:\n                # Some servers may stream raw text chunks\n                yield payload\n\n    def _format_http_error(self, resp: requests.Response, exc: Exception) -> str:\n        body = \"\"\n        try:\n            body = resp.text\n        except Exception:\n            pass\n        return f\"HTTP {resp.status_code}: {exc}\\n{body[:800]}\"\n\n    # ---- Legacy aliases: ASK-first ----\n    def ask(self, prompt_or_messages, **kw):\n        \"\"\"Legacy alias for .chat().\"\"\"\n        return self.chat(prompt_or_messages, **kw)\n\n    def query(self, prompt_or_messages, **kw):\n        \"\"\"Legacy alias for .chat().\"\"\"\n        return self.chat(prompt_or_messages, **kw)\n\n    def completions(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n    def completion(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n    def complete(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n    def chat_completions(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n\n# ---------------------------------------------------------------------------\n# Compatibility shim for older imports\n# ---------------------------------------------------------------------------\nclass TriAIInterface(AIInterface):\n    \"\"\"Backwards-compatible subclass \u2014 no changes needed.\"\"\"\n    pass\n\n__all__ = [\"AIInterface\", \"TriAIInterface\"]\n"
}