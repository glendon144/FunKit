{
  "id": 41,
  "title": "ai_interface_baseten",
  "body": "\"\"\"\nai_interface.py \u2014 Baseten (OpenAI-compatible) client\nRelease: v1.31\n\nUsage:\n  from modules.ai_interface import chat_once, stream_chat, quick_ask\n\n  # Non-stream (returns full string)\n  text = chat_once(\n      messages=[{\"role\": \"user\", \"content\": \"Say hi\"}],\n      model=None,            # optional override\n      temperature=0.7,\n      max_tokens=800\n  )\n\n  # Streaming (yield chunks)\n  for chunk in stream_chat(\n      messages=[{\"role\": \"user\", \"content\": \"Stream me a haiku\"}],\n      model=None,\n      temperature=0.8,\n  ):\n      print(chunk, end=\"\", flush=True)\n\nEnvironment:\n  BASETEN_API_KEY    (required)  \u2014 your Baseten key\n  BASETEN_BASE_URL   (optional)  \u2014 default \"https://inference.baseten.co/v1\"\n  BASETEN_MODEL      (optional)  \u2014 default \"openai/gpt-oss-120b\"\n  OPENAI_API_TIMEOUT (optional)  \u2014 seconds (float/int), default 60\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport time\nfrom typing import Generator, Iterable, List, Optional\n\n# Requires openai>=1.0.0\n# pip install --upgrade openai\nfrom openai import OpenAI\nfrom openai._exceptions import APIError, RateLimitError, APITimeoutError, APIConnectionError\n\n# --------------------------\n# Configuration / Defaults\n# --------------------------\n_DEFAULT_BASE_URL = os.getenv(\"BASETEN_BASE_URL\", \"https://inference.baseten.co/v1\")\n_DEFAULT_MODEL    = os.getenv(\"BASETEN_MODEL\", \"openai/gpt-oss-120b\")\n\n_API_KEY = os.getenv(\"BASETEN_API_KEY\")\n_TIMEOUT = float(os.getenv(\"OPENAI_API_TIMEOUT\", \"60\"))\n\n_client = OpenAI(\n    api_key=_API_KEY or \"MISSING_BASeTEN_API_KEY\",\n    base_url=_DEFAULT_BASE_URL,\n    timeout=_TIMEOUT,\n)\n\n# --------------------------\n# Core helpers\n# --------------------------\n\ndef _retryable(func, /, *, retries: int = 2, backoff: float = 0.8):\n    \"\"\"\n    Lightweight retry wrapper for transient network / rate limit issues.\n    \"\"\"\n    def _wrapped(*args, **kwargs):\n        delay = backoff\n        last_err = None\n        for attempt in range(retries + 1):\n            try:\n                return func(*args, **kwargs)\n            except (RateLimitError, APITimeoutError, APIConnectionError, APIError) as e:\n                last_err = e\n                if attempt >= retries:\n                    break\n                time.sleep(delay)\n                delay *= 2\n        raise last_err\n    return _wrapped\n\n\n@_retryable\ndef chat_once(\n    *,\n    messages: List[dict],\n    model: Optional[str] = None,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    max_tokens: int = 1000,\n    presence_penalty: float = 0.0,\n    frequency_penalty: float = 0.0,\n    stop: Optional[Iterable[str]] = None,\n    extra: Optional[dict] = None,\n) -> str:\n    \"\"\"\n    Non-streaming completion. Returns the assistant's full text.\n    \"\"\"\n    if not _API_KEY:\n        raise RuntimeError(\"BASETEN_API_KEY not set in environment\")\n\n    kwargs = dict(\n        model=model or _DEFAULT_MODEL,\n        messages=messages,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n    )\n    if stop:\n        kwargs[\"stop\"] = list(stop)\n    if extra:\n        kwargs.update(extra)\n\n    resp = _client.chat.completions.create(**kwargs)\n    # OpenAI-compatible response shape\n    if not resp.choices:\n        return \"\"\n    text = resp.choices[0].message.content or \"\"\n    return text\n\n\ndef stream_chat(\n    *,\n    messages: List[dict],\n    model: Optional[str] = None,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    max_tokens: int = 1000,\n    presence_penalty: float = 0.0,\n    frequency_penalty: float = 0.0,\n    stop: Optional[Iterable[str]] = None,\n    include_usage: bool = True,\n    continuous_usage_stats: bool = True,\n    extra: Optional[dict] = None,\n) -> Generator[str, None, None]:\n    \"\"\"\n    Streaming completion. Yields text deltas (str) as they arrive.\n    \"\"\"\n    if not _API_KEY:\n        raise RuntimeError(\"BASETEN_API_KEY not set in environment\")\n\n    kwargs = dict(\n        model=model or _DEFAULT_MODEL,\n        messages=messages,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        stream=True,\n        stream_options={\n            \"include_usage\": include_usage,\n            \"continuous_usage_stats\": continuous_usage_stats,\n        },\n    )\n    if stop:\n        kwargs[\"stop\"] = list(stop)\n    if extra:\n        kwargs.update(extra)\n\n    # minimal retry loop around the stream opener\n    opener = _retryable(_client.chat.completions.create)\n    response = opener(**kwargs)\n\n    for chunk in response:\n        try:\n            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n        except Exception:\n            # ignore malformed chunks but keep the stream alive\n            continue\n\n\n# --------------------------\n# Convenience wrappers\n# --------------------------\n\ndef quick_ask(\n    prompt: str,\n    *,\n    system: Optional[str] = None,\n    model: Optional[str] = None,\n    stream: bool = False,\n    **kw,\n):\n    \"\"\"\n    Quick one-off ask that builds messages array for you.\n\n    Returns:\n      - str if stream=False\n      - generator[str] if stream=True\n    \"\"\"\n    messages: List[dict] = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    if stream:\n        return stream_chat(messages=messages, model=model, **kw)\n    return chat_once(messages=messages, model=model, **kw)\n\n\n# --------------------------\n# Backwards-compat helpers for your CommandProcessor\n# --------------------------\n\ndef run_chat_for_processor(\n    selected_text: str,\n    *,\n    prefix: str = \"\",\n    system: Optional[str] = None,\n    model: Optional[str] = None,\n    stream: bool = False,\n    **kw,\n) -> str | Generator[str, None, None]:\n    \"\"\"\n    Typical call from CommandProcessor.query_ai:\n      result = run_chat_for_processor(sel, prefix=\"Please expand: \")\n    \"\"\"\n    user = (prefix or \"\") + selected_text\n    return quick_ask(user, system=system, model=model, stream=stream, **kw)\n\n"
}