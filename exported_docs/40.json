{
  "id": 40,
  "title": "ai_interface-cool",
  "body": "#!/usr/bin/env python3\n\"\"\"\nai_interface.py \u2014 Friendly OpenAI-compatible adapter for FunKit (LocalAI / vLLM / OpenAI)\n========================================================================================\n\nWhat's new vs. the previous version:\n- Robust HTTP retries with exponential backoff for 429/5xx\n- Friendly errors (short, user-facing) + verbose (debug) option\n- Health checks: .ping() and .whoami() (reports server + model details)\n- Model discovery: .list_models() (works with OpenAI/LocalAI/vLLM)\n- Safer streaming: .stream_text() (yields tokens) and .stream_json() (raw SSE)\n- Timeouts per-call override; sensible defaults from env\n- Extra helpers: .set_headers(), .with_options(), .raw() (low-level access)\n- Fully backward compatible with legacy ASK-first aliases\n\nEnvironment (with defaults):\n  PIKIT_OPENAI_BASE_URL   default: http://localhost:8080/v1\n  PIKIT_OPENAI_API_KEY    default: sk-local\n  PIKIT_MODEL_NAME        default: mistral-7b-instruct\n  PIKIT_REQUEST_TIMEOUT   default: 120\n  PIKIT_CHAT_TEMPERATURE  default: 0.7\n  PIKIT_RETRY_TOTAL       default: 4\n  PIKIT_RETRY_BACKOFF     default: 0.5\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os, json, typing as t\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\nJson = t.Dict[str, t.Any]\nHeaders = t.Dict[str, str]\n\nDEFAULTS = {\n    \"base_url\": os.getenv(\"PIKIT_OPENAI_BASE_URL\", \"http://localhost:8080/v1\").rstrip(\"/\"),\n    \"api_key\": os.getenv(\"PIKIT_OPENAI_API_KEY\", \"sk-local\"),\n    \"model\": os.getenv(\"PIKIT_MODEL_NAME\", \"mistral-7b-instruct\"),\n    \"timeout\": float(os.getenv(\"PIKIT_REQUEST_TIMEOUT\", \"120\")),\n    \"temperature\": float(os.getenv(\"PIKIT_CHAT_TEMPERATURE\", \"0.7\")),\n    \"retry_total\": int(os.getenv(\"PIKIT_RETRY_TOTAL\", \"4\")),\n    \"retry_backoff\": float(os.getenv(\"PIKIT_RETRY_BACKOFF\", \"0.5\")),\n}\n\ndef _as_messages(user_or_messages: t.Union[str, t.List[Json]], system_prompt: t.Optional[str]) -> t.List[Json]:\n    if isinstance(user_or_messages, str):\n        msgs = [{\"role\": \"user\", \"content\": user_or_messages}]\n    else:\n        msgs = list(user_or_messages)\n    if system_prompt:\n        if not msgs or msgs[0].get(\"role\") != \"system\":\n            msgs = [{\"role\": \"system\", \"content\": system_prompt}] + msgs\n    return msgs\n\nclass AIInterface:\n    \"\"\"\n    Usage:\n        ai = AIInterface()\n        text = ai.ask(\"Say hi.\")              # legacy alias (chat)\n        text = ai.chat(\"Say hi.\")             # modern\n        for tok in ai.chat(\"stream me\", stream=True): print(tok, end=\"\")\n        vecs = ai.embed([\"hello\", \"world\"])\n        print(ai.ping(), ai.whoami(), ai.list_models())\n    \"\"\"\n    def __init__(self,\n                 base_url: str | None = None,\n                 api_key: str | None = None,\n                 model: str | None = None,\n                 timeout: float | None = None,\n                 default_temperature: float | None = None,\n                 retry_total: int | None = None,\n                 retry_backoff: float | None = None,\n                 debug: bool = False):\n        cfg = DEFAULTS.copy()\n        if base_url: cfg[\"base_url\"] = base_url.rstrip(\"/\")\n        if api_key: cfg[\"api_key\"] = api_key\n        if model: cfg[\"model\"] = model\n        if timeout is not None: cfg[\"timeout\"] = timeout\n        if default_temperature is not None: cfg[\"temperature\"] = default_temperature\n        if retry_total is not None: cfg[\"retry_total\"] = retry_total\n        if retry_backoff is not None: cfg[\"retry_backoff\"] = retry_backoff\n\n        self.base_url = cfg[\"base_url\"]\n        self.api_key = cfg[\"api_key\"]\n        self.model = cfg[\"model\"]\n        self.timeout = cfg[\"timeout\"]\n        self.default_temperature = cfg[\"temperature\"]\n        self.debug = bool(debug)\n\n        self._session = requests.Session()\n        self._system_prompt: str | None = None\n        self._extra_headers: Headers = {}\n\n        # Install retries (idempotent POSTs to LLMs are generally safe to retry)\n        retry = Retry(\n            total=cfg[\"retry_total\"],\n            backoff_factor=cfg[\"retry_backoff\"],\n            status_forcelist=(429, 500, 502, 503, 504),\n            allowed_methods=frozenset([\"GET\", \"POST\"]),\n            raise_on_status=False,\n        )\n        adapter = HTTPAdapter(max_retries=retry)\n        self._session.mount(\"http://\", adapter)\n        self._session.mount(\"https://\", adapter)\n\n    # ---- config ----\n    def set_system_prompt(self, text: str | None) -> None:\n        self._system_prompt = text or None\n\n    def set_headers(self, headers: Headers | None) -> None:\n        \"\"\"Set/replace extra headers to be merged on every request.\"\"\"\n        self._extra_headers = dict(headers or {})\n\n    def with_options(self, **overrides) -> \"AIInterface\":\n        \"\"\"Return a shallow clone with a few overrides (e.g., model='\u2026').\"\"\"\n        cls = self.__class__\n        new = cls(\n            base_url=overrides.get(\"base_url\", self.base_url),\n            api_key=overrides.get(\"api_key\", self.api_key),\n            model=overrides.get(\"model\", self.model),\n            timeout=overrides.get(\"timeout\", self.timeout),\n            default_temperature=overrides.get(\"default_temperature\", self.default_temperature),\n            retry_total=overrides.get(\"retry_total\", DEFAULTS[\"retry_total\"]),\n            retry_backoff=overrides.get(\"retry_backoff\", DEFAULTS[\"retry_backoff\"]),\n            debug=overrides.get(\"debug\", self.debug),\n        )\n        new._system_prompt = self._system_prompt\n        new._extra_headers = dict(self._extra_headers)\n        return new\n\n    def get_config(self) -> Json:\n        return {\n            \"base_url\": self.base_url,\n            \"model\": self.model,\n            \"timeout\": self.timeout,\n            \"default_temperature\": self.default_temperature,\n            \"has_system_prompt\": bool(self._system_prompt),\n            \"has_extra_headers\": bool(self._extra_headers),\n            \"debug\": self.debug,\n        }\n\n    # ---- utilities ----\n    def _headers(self, extra_headers: Headers | None = None) -> Headers:\n        h = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        if self._extra_headers:\n            h.update(self._extra_headers)\n        if extra_headers:\n            h.update(extra_headers)\n        return h\n\n    def _format_http_error(self, resp: requests.Response, exc: Exception) -> str:\n        try:\n            body = resp.text\n        except Exception:\n            body = \"\"\n        short = f\"HTTP {resp.status_code}: {exc}\"\n        if not self.debug:\n            # Try to compress noisy backend JSON into a single line\n            return f\"{short}\\n{body[:800]}\"\n        return f\"{short}\\n{body}\"\n\n    def _extract_text(self, resp: requests.Response) -> str:\n        try:\n            resp.raise_for_status()\n        except Exception as e:\n            raise RuntimeError(self._format_http_error(resp, e))\n        try:\n            data = resp.json()\n        except Exception as e:\n            raise RuntimeError(f\"Bad JSON from server: {e}\\n{resp.text[:800]}\")\n        # Chat primary\n        try:\n            return data[\"choices\"][0][\"message\"][\"content\"] or \"\"\n        except Exception:\n            # Completion fallback\n            try:\n                return data[\"choices\"][0].get(\"text\", \"\") or \"\"\n            except Exception:\n                raise RuntimeError(f\"Unexpected response: {json.dumps(data)[:600]}\")\n\n    def _stream_text_from_resp(self, resp: requests.Response) -> t.Iterator[str]:\n        for line in resp.iter_lines(decode_unicode=True):\n            if not line:\n                continue\n            payload = line[6:].strip() if line.startswith(\"data: \") else line.strip()\n            if payload == \"[DONE]\":\n                break\n            try:\n                j = json.loads(payload)\n                delta = j.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n                if delta:\n                    yield delta\n            except Exception:\n                # Some servers may stream raw text chunks\n                yield payload\n\n    # ---- health & discovery ----\n    def ping(self, timeout: float | None = None) -> bool:\n        \"\"\"Quick health probe against base_url (returns True if HTTP 200..399).\"\"\"\n        try:\n            resp = self._session.get(self.base_url, headers=self._headers(), timeout=timeout or self.timeout)\n            return 200 <= resp.status_code < 400\n        except Exception:\n            return False\n\n    def whoami(self) -> Json:\n        \"\"\"Return a small dict describing the server and default model.\"\"\"\n        info = {\"base_url\": self.base_url, \"model\": self.model}\n        try:\n            # Try /models (works with OpenAI/LocalAI)\n            ms = self.list_models()\n            info[\"models_count\"] = len(ms)\n            info[\"has_default_model\"] = any(m.get(\"id\") == self.model for m in ms)\n        except Exception as e:\n            info[\"models_error\"] = str(e)\n        return info\n\n    def list_models(self) -> t.List[Json]:\n        url = f\"{self.base_url}/models\"\n        resp = self._session.get(url, headers=self._headers(), timeout=self.timeout)\n        resp.raise_for_status()\n        data = resp.json()\n        # OpenAI shape: {\"data\":[{\"id\":\"gpt-3.5-turbo\",...},...]}\n        if isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n            return data[\"data\"]\n        # LocalAI can return a plain list sometimes\n        if isinstance(data, list):\n            return data\n        return []\n\n    # ---- chat ----\n    def chat(self,\n             prompt_or_messages: t.Union[str, t.List[Json]],\n             stream: bool = False,\n             temperature: float | None = None,\n             max_tokens: int | None = None,\n             tools: t.Optional[t.List[Json]] = None,\n             tool_choice: t.Optional[t.Union[str, Json]] = None,\n             stop: t.Optional[t.Union[str, t.List[str]]] = None,\n             extra_headers: t.Optional[Headers] = None,\n             timeout: float | None = None,\n             **extra) -> t.Union[str, t.Iterator[str]]:\n        url = f\"{self.base_url}/chat/completions\"\n        msgs = _as_messages(prompt_or_messages, self._system_prompt)\n        payload: Json = {\n            \"model\": self.model,\n            \"messages\": msgs,\n            \"temperature\": self.default_temperature if temperature is None else temperature,\n        }\n        if max_tokens is not None: payload[\"max_tokens\"] = max_tokens\n        if tools is not None: payload[\"tools\"] = tools\n        if tool_choice is not None: payload[\"tool_choice\"] = tool_choice\n        if stop is not None: payload[\"stop\"] = stop\n        if extra: payload.update(extra)\n\n        headers = self._headers(extra_headers)\n        to = timeout or self.timeout\n\n        if stream:\n            payload[\"stream\"] = True\n            resp = self._session.post(url, headers=headers, json=payload, timeout=to, stream=True)\n            try:\n                resp.raise_for_status()\n            except Exception as e:\n                raise RuntimeError(self._format_http_error(resp, e))\n            return self._stream_text_from_resp(resp)\n        else:\n            resp = self._session.post(url, headers=headers, json=payload, timeout=to)\n            return self._extract_text(resp)\n\n    # Convenience wrappers for streaming explicitness\n    def stream_text(self, prompt_or_messages: t.Union[str, t.List[Json]], **kw) -> t.Iterator[str]:\n        kw[\"stream\"] = True\n        return self.chat(prompt_or_messages, **kw)  # type: ignore[return-value]\n\n    def stream_json(self, prompt_or_messages: t.Union[str, t.List[Json]], **kw) -> t.Iterator[Json]:\n        \"\"\"Yield raw SSE JSON dicts (if server sends JSON events).\"\"\"\n        url = f\"{self.base_url}/chat/completions\"\n        msgs = _as_messages(prompt_or_messages, self._system_prompt)\n        payload: Json = {\n            \"model\": self.model,\n            \"messages\": msgs,\n            \"temperature\": self.default_temperature if kw.get(\"temperature\") is None else kw[\"temperature\"],\n            \"stream\": True,\n        }\n        for k in (\"max_tokens\", \"tools\", \"tool_choice\", \"stop\"):\n            if k in kw and kw[k] is not None:\n                payload[k] = kw[k]\n        headers = self._headers(kw.get(\"extra_headers\"))\n        to = kw.get(\"timeout\", self.timeout)\n        resp = self._session.post(url, headers=headers, json=payload, timeout=to, stream=True)\n        try:\n            resp.raise_for_status()\n        except Exception as e:\n            raise RuntimeError(self._format_http_error(resp, e))\n        for line in resp.iter_lines(decode_unicode=True):\n            if not line:\n                continue\n            payload = line[6:].strip() if line.startswith(\"data: \") else line.strip()\n            if payload == \"[DONE]\":\n                break\n            try:\n                yield json.loads(payload)\n            except Exception:\n                # Provide a consistent JSON wrapper if server emits raw text\n                yield {\"type\": \"text\", \"data\": payload}\n\n    # ---- embeddings ----\n    def embed(self, texts: t.List[str], model: str | None = None, timeout: float | None = None) -> t.List[t.List[float]]:\n        url = f\"{self.base_url}/embeddings\"\n        payload: Json = {\n            \"model\": model or self.model,\n            \"input\": texts,\n        }\n        resp = self._session.post(url, headers=self._headers(), json=payload, timeout=timeout or self.timeout)\n        resp.raise_for_status()\n        data = resp.json()\n        return [item[\"embedding\"] for item in data.get(\"data\", [])]\n\n    # ---- raw access (power users) ----\n    def raw(self, path: str, method: str = \"GET\", json_body: Json | None = None,\n            headers: Headers | None = None, **req_kw) -> requests.Response:\n        url = path if path.startswith(\"http\") else f\"{self.base_url.rstrip('/')}/{path.lstrip('/')}\"\n        h = self._headers(headers)\n        m = method.upper()\n        func = self._session.get if m == \"GET\" else self._session.post\n        return func(url, headers=h, json=json_body, timeout=req_kw.pop(\"timeout\", self.timeout), **req_kw)\n\n    # ---- Legacy aliases: ASK-first ----\n    def ask(self, prompt_or_messages, **kw):\n        \"\"\"Legacy alias for .chat().\"\"\"\n        return self.chat(prompt_or_messages, **kw)\n\n    def query(self, prompt_or_messages, **kw):\n        \"\"\"Legacy alias for .chat().\"\"\"\n        return self.chat(prompt_or_messages, **kw)\n\n    def completions(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n    def completion(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n    def complete(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n    def chat_completions(self, prompt_or_messages, **kw):\n        return self.chat(prompt_or_messages, **kw)\n\n\n# ---------------------------------------------------------------------------\n# Compatibility shim for older imports\n# ---------------------------------------------------------------------------\nclass TriAIInterface(AIInterface):\n    \"\"\"Backwards-compatible subclass \u2014 no changes needed.\"\"\"\n    pass\n\n__all__ = [\"AIInterface\", \"TriAIInterface\"]\n"
}