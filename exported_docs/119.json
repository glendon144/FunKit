{
  "id": 119,
  "title": "opml_crawler_adapter",
  "body": "# modules/opml_crawler_adapter.py\nfrom __future__ import annotations\n\nimport re\nimport xml.etree.ElementTree as ET\nfrom typing import Callable, Tuple, List, Set, Optional\nfrom urllib.parse import urljoin\n\n# Try requests; fall back to urllib if unavailable\ntry:\n    import requests\n    _HAVE_REQUESTS = True\nexcept Exception:  # pragma: no cover\n    import urllib.request  # type: ignore\n    _HAVE_REQUESTS = False\n\n\nDEFAULT_HEADERS = {\n    \"User-Agent\": \"FunKit-OPML-Crawler/1.0 (+funkit)\",\n    \"Accept\": \"text/xml, application/xml, text/html, */*\",\n}\n\n\ndef _fetch_url(url: str, timeout: int = 15) -> Optional[str]:\n    \"\"\"Fetch URL and return decoded text (utf-8 with replace).\"\"\"\n    try:\n        if _HAVE_REQUESTS:\n            r = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout)\n            r.raise_for_status()\n            # Some servers mislabel content-type; we still read text as-is.\n            return r.text\n        else:\n            req = urllib.request.Request(url, headers=DEFAULT_HEADERS)  # type: ignore[attr-defined]\n            with urllib.request.urlopen(req, timeout=timeout) as resp:  # type: ignore[attr-defined]\n                data = resp.read()\n            return data.decode(\"utf-8\", errors=\"replace\")\n    except Exception as e:\n        print(f\"[opml_crawler] fetch failed {url}: {e}\")\n        return None\n\n\ndef _load_local(path: str) -> Optional[str]:\n    try:\n        with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            return f.read()\n    except Exception as e:\n        print(f\"[opml_crawler] read failed {path}: {e}\")\n        return None\n\n\ndef _is_opml(text: str) -> bool:\n    \"\"\"Cheap OPML check: parse root and verify <opml>.\"\"\"\n    if not text:\n        return False\n    try:\n        root = ET.fromstring(text)\n        return (root.tag or \"\").lower().endswith(\"opml\")\n    except ET.ParseError:\n        return False\n\n\ndef _iter_opml_links_from_xml(xml_text: str, base_url: Optional[str] = None) -> List[str]:\n    \"\"\"\n    Extract likely OPML links from <outline> elements:\n      - xmlUrl=\"<url>\"\n      - url=\"<url>\" (when it ends with .opml)\n      - type=\"link\" with url ending in .opml\n    \"\"\"\n    links: List[str] = []\n    try:\n        root = ET.fromstring(xml_text)\n    except ET.ParseError as e:\n        print(f\"[opml_crawler] parse error: {e}\")\n        return links\n\n    for el in root.findall(\".//outline\"):\n        attrs = el.attrib\n        url = attrs.get(\"xmlUrl\") or attrs.get(\"url\")\n        typ = (attrs.get(\"type\") or \"\").lower()\n        if not url:\n            continue\n        url_l = url.lower()\n        if url_l.endswith(\".opml\") or (typ == \"link\" and url_l.endswith(\".opml\")):\n            links.append(urljoin(base_url, url) if base_url else url)\n    return links\n\n\ndef _iter_opml_links_from_html(html_text: str, base_url: Optional[str] = None) -> List[str]:\n    \"\"\"\n    Very small, dependency-free extractor:\n    find href=\"...opml\" and resolve relative links.\n    \"\"\"\n    links: List[str] = []\n    for m in re.finditer(r'href\\s*=\\s*[\"\\']([^\"\\']+?\\.opml)([#?\"\\']|$)', html_text, flags=re.IGNORECASE):\n        url = m.group(1)\n        links.append(urljoin(base_url, url) if base_url else url)\n    return links\n\n\ndef crawl_opml(\n    start: str,\n    max_depth: int = 3,\n    visited: Optional[Set[str]] = None,\n) -> List[Tuple[str, str]]:\n    \"\"\"\n    Crawl an entry point (URL or local file) and gather text payloads.\n    Returns a flat list of (source, payload_text).\n\n    Notes:\n      - If the payload is OPML, we recurse into OPML links inside it.\n      - If the payload is HTML, we ALSO recurse into any .opml links found in the HTML.\n      - This function does NOT write to the database and does NOT convert to OPML.\n        Let the GUI/CLI call aopmlengine.convert_payload_to_opml(...) on each payload.\n    \"\"\"\n    if visited is None:\n        visited = set()\n\n    key = start.strip()\n    if key in visited:\n        return []\n    visited.add(key)\n\n    # Load text\n    if key.startswith((\"http://\", \"https://\")):\n        payload = _fetch_url(key)\n        base_url = key\n    else:\n        payload = _load_local(key)\n        base_url = None\n\n    if not payload:\n        return []\n\n    results: List[Tuple[str, str]] = [(key, payload)]\n\n    if max_depth <= 0:\n        return results\n\n    # Recurse based on what we have\n    if _is_opml(payload):\n        children = _iter_opml_links_from_xml(payload, base_url)\n    else:\n        children = _iter_opml_links_from_html(payload, base_url)\n\n    for child_url in children:\n        if child_url in visited:\n            continue\n        results.extend(crawl_opml(child_url, max_depth=max_depth - 1, visited=visited))\n\n    return results\n\n\ndef crawl_and_import(\n    start: str,\n    import_fn: Callable[[str, str], None],\n    max_depth: int = 3,\n) -> List[Tuple[str, str]]:\n    \"\"\"\n    Crawl and invoke `import_fn(source, payload_text)` for each gathered item.\n    WARNING: If you use this from a GUI thread, schedule the DB writes on the Tk thread\n    (e.g., via `self.after(0, ...)`) to avoid sqlite cross-thread errors.\n    \"\"\"\n    gathered = crawl_opml(start, max_depth=max_depth)\n    for src, payload_text in gathered:\n        try:\n            import_fn(src, payload_text)\n        except Exception as e:\n            print(f\"[opml_crawler] import failed for {src}: {e}\")\n    return gathered\n\n"
}