from __future__ import annotations
import modules.ai_interface_baseten as b10
import os, json, time
from typing import List, Dict, Any
import urllib.request, urllib.error
from .provider_registry import registry
from modules.safe_text import sanitize_if_needed

class AIInterface:
    def __init__(self, provider_key: str | None = None):
                # Default to the UI-selected provider when not specified
        try:
            if provider_key is None:
                provider_key = registry.read_selected()
        except Exception:
            pass
        self.provider = registry.get(provider_key)



    def _headers(self) -> Dict[str, str]:
        if self.provider.key == "openai":
            token = os.getenv(self.provider.env_key or "", "")
            return {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
        if self.provider.key == "baseten":
            token = os.getenv(self.provider.env_key or "", "")
            return {"Authorization": f"Api-Key {token}", "Content-Type": "application/json"}
        if self.provider.key in ("local_llama", "hf_tgi"):
            return {"Content-Type":"application/json"}
        return {"Content-Type":"application/json"}

    def _url(self) -> str:
        if self.provider.key == "openai":
            # Compatible “/chat/completions” route
            return "https://api.openai.com/v1/chat/completions"
        if self.provider.key == "baseten":
            # You may need to append model id; left generic here.
            return f"{self.provider.endpoint.rstrip('/')}/chat/completions"
        if self.provider.key == "local_llama":
            # llama.cpp server with OpenAI-compatible route
            return f"{self.provider.endpoint.rstrip('/')}/chat/completions"
        if self.provider.key == "hf_tgi":
            # Basic TGI compat via /v1/chat/completions proxy (if you run a shim)
            return f"{self.provider.endpoint.rstrip('/')}/v1/chat/completions"
        return "http://127.0.0.1:8080/v1/chat/completions"

    def chat(self, messages: List[Dict[str, Any]], temperature: float = 0.7, max_tokens: int = 512) -> str:
        # Normalize payload to OpenAI Chat Completions schema
        payload = {
            "model": self.provider.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        req = urllib.request.Request(self._url(), data=json.dumps(payload).encode("utf-8"), headers=self._headers(), method="POST")
        try:
            with urllib.request.urlopen(req, timeout=self.provider.extras.get("timeout", 60)) as resp:
                data = json.loads(resp.read().decode("utf-8"))
        except urllib.error.HTTPError as e:
            return f"[{self.provider.label}] HTTPError {e.code}: {e.read().decode('utf-8', errors='ignore')}"
        except Exception as e:
            return f"[{self.provider.label}] Error: {e}"

        # Try to extract text in a generic manner
        txt = ""
        try:
            choices = data.get("choices", [])
            if choices and "message" in choices[0]:
                txt = choices[0]["message"].get("content", "")
            elif "generated_text" in data:
                txt = data["generated_text"]
        except Exception:
            pass
        return txt or "[No content returned]"

    # ---- Back-compat convenience wrappers ----
    def query(self, prompt: str, *, system: str = "You are a helpful assistant.", max_tokens: int = 512, temperature: float = 0.7) -> str:
        """Legacy FunKit/DemoKit style: ai.query("..."). Routes to chat() under the hood."""
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ]
        return self.chat(messages, temperature=temperature, max_tokens=max_tokens)

    def complete(self, prompt: str, *, max_tokens: int = 512, temperature: float = 0.7) -> str:
        """Another common legacy alias some modules used. Equivalent to query() with a generic system prompt."""
        return self.query(prompt, system="You are a helpful assistant.", max_tokens=max_tokens, temperature=temperature)
